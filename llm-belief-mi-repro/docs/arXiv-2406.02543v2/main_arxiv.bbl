\begin{thebibliography}{62}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Ahdritz et~al.(2024)Ahdritz, Qin, Vyas, Barak, and
  Edelman]{ahdritz2024}
Gustaf Ahdritz, Tian Qin, Nikhil Vyas, Boaz Barak, and Benjamin~L. Edelman.
\newblock Distinguishing the knowable from the unknowable with language models,
  2024.
\newblock URL \url{https://arxiv.org/abs/2402.03563}.

\bibitem[Angelopoulos et~al.(2023)Angelopoulos, Bates,
  et~al.]{angelopoulos2023conformal}
Anastasios~N Angelopoulos, Stephen Bates, et~al.
\newblock Conformal prediction: A gentle introduction.
\newblock \emph{Foundations and Trends{\textregistered} in Machine Learning},
  16\penalty0 (4):\penalty0 494--591, 2023.

\bibitem[Antor{\'a}n et~al.(2020)Antor{\'a}n, Allingham, and
  Hern{\'a}ndez-Lobato]{antoran2020depth}
Javier Antor{\'a}n, James Allingham, and Jos{\'e}~Miguel Hern{\'a}ndez-Lobato.
\newblock Depth uncertainty in neural networks.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  2020.

\bibitem[Azaria and Mitchell(2023)]{azaria2023internal}
Amos Azaria and Tom Mitchell.
\newblock The internal state of an {LLM} knows when its lying.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2023.

\bibitem[Berend and Kontorovich(2012)]{berend2012missing}
Daniel Berend and Aryeh Kontorovich.
\newblock The missing mass problem.
\newblock \emph{Statistics \& Probability Letters}, 82\penalty0 (6):\penalty0
  1102--1110, 2012.

\bibitem[Berend and Kontorovich(2013)]{berend2013concentration}
Daniel Berend and Aryeh Kontorovich.
\newblock {On the concentration of the missing mass}.
\newblock \emph{Electronic Communications in Probability}, 2013.

\bibitem[Berend et~al.(2017)Berend, Kontorovich, and
  Zagdanski]{berend2017expected}
Daniel Berend, Aryeh Kontorovich, and Gil Zagdanski.
\newblock The expected missing mass under an entropy constraint.
\newblock \emph{Entropy}, 19\penalty0 (7):\penalty0 315, 2017.

\bibitem[Blundell et~al.(2015)Blundell, Cornebise, Kavukcuoglu, and
  Wierstra]{blundell2015weight}
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra.
\newblock Weight uncertainty in neural network.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2015.

\bibitem[Bubeck et~al.(2023)Bubeck, Chandrasekaran, Eldan, Gehrke, Horvitz,
  Kamar, Lee, Lee, Li, Lundberg, Nori, Palangi, Ribeiro, and
  Zhang]{bubeck2023sparks}
SÃ©bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric
  Horvitz, Ece Kamar, Peter Lee, Yin~Tat Lee, Yuanzhi Li, Scott Lundberg,
  Harsha Nori, Hamid Palangi, Marco~Tulio Ribeiro, and Yi~Zhang.
\newblock Sparks of artificial general intelligence: Early experiments with
  gpt-4.
\newblock \emph{arXiv preprint arXiv:2303.12712}, 2023.

\bibitem[Burns et~al.(2023)Burns, Ye, Klein, and Steinhardt]{BYKS-2023}
Collin Burns, Haotian Ye, Dan Klein, and Jacob Steinhardt.
\newblock Discovering latent knowledge in language models without supervision.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Cesa-Bianchi and Lugosi(2006)]{cesa2006prediction}
Nicolo Cesa-Bianchi and G{\'a}bor Lugosi.
\newblock \emph{Prediction, learning, and games}.
\newblock Cambridge University Press, 2006.

\bibitem[Chen et~al.(2024{\natexlab{a}})Chen, Liu, Chen, Gu, Wu, Tao, Fu, and
  Ye]{chen2024inside}
Chao Chen, Kai Liu, Ze~Chen, Yi~Gu, Yue Wu, Mingyuan Tao, Zhihang Fu, and
  Jieping Ye.
\newblock {INSIDE}: {LLM}s' internal states retain the power of hallucination
  detection.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2024{\natexlab{a}}.

\bibitem[Chen et~al.(2024{\natexlab{b}})Chen, Xiong, Liu, Wu, Xiao, Gao, and
  He]{chen2024incontext}
Shiqi Chen, Miao Xiong, Junteng Liu, Zhengxuan Wu, Teng Xiao, Siyang Gao, and
  Junxian He.
\newblock In-context sharpness as alerts: An inner representation perspective
  for hallucination mitigation.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024{\natexlab{b}}.
\newblock URL \url{https://openreview.net/forum?id=s3e8poX3kb}.

\bibitem[Chen et~al.(2023)Chen, Aksitov, Alon, Ren, Xiao, Yin, Prakash, Sutton,
  Wang, and Zhou]{chen2023universalselfconsistencylargelanguage}
Xinyun Chen, Renat Aksitov, Uri Alon, Jie Ren, Kefan Xiao, Pengcheng Yin,
  Sushant Prakash, Charles Sutton, Xuezhi Wang, and Denny Zhou.
\newblock Universal self-consistency for large language model generation, 2023.

\bibitem[Cole et~al.(2023)Cole, Zhang, Gillick, Eisenschlos, Dhingra, and
  Eisenstein]{CZGEDE2023}
Jeremy~R. Cole, Michael~JQ Zhang, Daniel Gillick, Julian~Martin Eisenschlos,
  Bhuwan Dhingra, and Jacob Eisenstein.
\newblock Selectively answering ambiguous questions.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2023.

\bibitem[Devlin et~al.(2019)Devlin, Chang, Lee, and Toutanova]{devlin2019bert}
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.
\newblock {BERT}: Pre-training of deep bidirectional transformers for language
  understanding.
\newblock \emph{arXiv preprint arXiv:1810.04805}, 2019.

\bibitem[Dwaracherla et~al.(2023)Dwaracherla, Wen, Osband, Lu, Asghari, and
  Roy]{dwaracherla2023ensembles}
Vikranth Dwaracherla, Zheng Wen, Ian Osband, Xiuyuan Lu, Seyed~Mohammad
  Asghari, and Benjamin~Van Roy.
\newblock Ensembles for uncertainty estimation: Benefits of prior functions and
  bootstrapping.
\newblock \emph{Transactions on Machine Learning Research (TMLR)}, 2023.
\newblock ISSN 2835-8856.

\bibitem[Farquhar et~al.(2024)Farquhar, Kossen, Kuhn, and Gal]{FKKG-2024}
S.~Farquhar, J.~Kossen, L.~Kuhn, and Yarin Gal.
\newblock Detecting hallucinations in large language models using semantic
  entropy.
\newblock 2024.

\bibitem[Fellbaum(1998)]{fellbaum1998wordnet}
Christiane Fellbaum.
\newblock \emph{WordNet: An electronic lexical database}.
\newblock MIT press, 1998.

\bibitem[Gal(2016)]{Gal-2016}
Yarin Gal.
\newblock \emph{Uncertainty in deep learning}.
\newblock PhD thesis, University of Cambridge, 2016.

\bibitem[Gale and Sampson(1995)]{gale1995good}
William~A Gale and Geoffrey Sampson.
\newblock Good-turing frequency estimation without tears.
\newblock \emph{Journal of quantitative linguistics}, 2\penalty0 (3):\penalty0
  217--237, 1995.

\bibitem[{Gemini Team, Google}(2023)]{geminiteam2023gemini}
{Gemini Team, Google}.
\newblock Gemini: A family of highly capable multimodal models.
\newblock \emph{arXiv preprint arXiv:2312.11805}, 2023.
\newblock [Online; accessed 01-February-2024].

\bibitem[Hou et~al.(2024)Hou, Liu, Qian, Andreas, Chang, and
  Zhang]{hou2023decomposing}
Bairu Hou, Yujian Liu, Kaizhi Qian, Jacob Andreas, Shiyu Chang, and Yang Zhang.
\newblock Decomposing uncertainty for large language models through input
  clarification ensembling.
\newblock In \emph{International Conference on Machine Learing (ICML)}, 2024.

\bibitem[Jiang et~al.(2024)Jiang, Ruan, Huang, Liao, Pitis, Grosse, and
  Ba]{JRHLPGB-2023}
Mingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei Liao, Silviu Pitis, Roger
  Grosse, and Jimmy Ba.
\newblock Calibrating language models via augmented prompt ensembles.
\newblock In \emph{Workshop on Challenges in Deployable Generative AI at
  International Conference on Machine Learning}, 2024.

\bibitem[Johnson et~al.(2024)Johnson, Tarlow, Duvenaud, and
  Maddison]{johnson2024experts}
Daniel~D. Johnson, Daniel Tarlow, David Duvenaud, and Chris~J. Maddison.
\newblock Experts don't cheat: Learning what you don't know by predicting
  pairs.
\newblock \emph{arXiv preprint arXiv:2402.08733}, 2024.

\bibitem[Joshi et~al.(2017)Joshi, Choi, Weld, and
  Zettlemoyer]{joshi2017triviaqa}
Mandar Joshi, Eunsol Choi, Daniel~S Weld, and Luke Zettlemoyer.
\newblock {TriviaQA}: A large scale distantly supervised challenge dataset for
  reading comprehension.
\newblock In \emph{Transactions of the Association for Computational
  Linguistics (ACL)}, pages 1601--1611, 2017.

\bibitem[Kadavath et~al.(2022)Kadavath, Conerly, Askell, Henighan, Drain,
  Perez, Schiefer, Dodds, DasSarma, Tran-Johnson, and et~al]{KCAHD2022}
Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan
  Perez, Nicholas Schiefer, Zac~Hatfield Dodds, Nova DasSarma, Eli
  Tran-Johnson, and et~al.
\newblock Language models (mostly) know what they know.
\newblock \emph{arXiv preprint arXiv:2207.05221}, 2022.

\bibitem[Kassner and Sch\"{u}tze(2020)]{KS-2020}
Nora Kassner and Hinrich Sch\"{u}tze.
\newblock Negated and misprimed probes for pretrained language models: Birds
  can talk, but cannot fly.
\newblock In \emph{Transactions of the Association for Computational
  Linguistics (ACL)}, 2020.

\bibitem[Krishna(2023)]{krishna2023intersectionselfcorrectiontrustlanguage}
Satyapriya Krishna.
\newblock On the intersection of self-correction and trust in language models,
  2023.
\newblock URL \url{https://arxiv.org/abs/2311.02801}.

\bibitem[Krishna et~al.(2024)Krishna, Agarwal, and
  Lakkaraju]{krishna2024understanding}
Satyapriya Krishna, Chirag Agarwal, and Himabindu Lakkaraju.
\newblock Understanding the effects of iterative prompting on truthfulness.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.
\newblock URL \url{https://openreview.net/forum?id=KjazcKPMME}.

\bibitem[Kuhn et~al.(2023)Kuhn, Gal, and Farquhar]{KuhnARXIV2023}
Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar.
\newblock Semantic uncertainty: Linguistic invariances for uncertainty
  estimation in natural language generation.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2023.

\bibitem[Laban et~al.(2024)Laban, Murakhovs'ka, Xiong, and
  Wu]{laban2024surechallengingllmsleads}
Philippe Laban, Lidiya Murakhovs'ka, Caiming Xiong, and Chien-Sheng Wu.
\newblock Are you sure? challenging llms leads to performance drops in the
  flipflop experiment, 2024.
\newblock URL \url{https://arxiv.org/abs/2311.08596}.

\bibitem[Lakshminarayanan et~al.(2017{\natexlab{a}})Lakshminarayanan, Pritzel,
  and Blundell]{LPB2017}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, volume~30, 2017{\natexlab{a}}.

\bibitem[Lakshminarayanan et~al.(2017{\natexlab{b}})Lakshminarayanan, Pritzel,
  and Blundell]{lakshminarayanan2017simple}
Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell.
\newblock Simple and scalable predictive uncertainty estimation using deep
  ensembles.
\newblock \emph{Conference on Neural Information Processing Systems (NeurIPS)},
  2017{\natexlab{b}}.

\bibitem[Li et~al.(2023)Li, Rawat, Zaheer, Wang, Lukasik, Veit, Yu, and
  Kumar]{LRZWLVYK-2023}
Daliang Li, Ankit~Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas
  Veit, Felix Yu, and Sanjiv Kumar.
\newblock Large language models with controllable working memory.
\newblock In \emph{Transactions of the Association for Computational
  Linguistics (ACL)}, 2023.

\bibitem[Li et~al.(2024)Li, Wang, Feng, Zhu, Wang, and Chua]{li2024thinktwice}
Moxin Li, Wenjie Wang, Fuli Feng, Fengbin Zhu, Qifan Wang, and Tat-Seng Chua.
\newblock Think twice before trusting: Self-detection for large language models
  through comprehensive answer reflection, 2024.
\newblock URL \url{https://arxiv.org/abs/2403.09972}.

\bibitem[Lin et~al.(2023)Lin, Trivedi, and Sun]{LTS2023}
Zhen Lin, Shubhendu Trivedi, and Jimeng Sun.
\newblock Generating with confidence: Uncertainty quantification for black-box
  large language models.
\newblock \emph{arXiv preprint arXiv:2305.19187}, 2023.

\bibitem[Longpre et~al.(2021)Longpre, Perisetla, Chen, Ramesh, DuBois, and
  Singh]{LPCRDS-2021}
Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois,
  and Sameer Singh.
\newblock Entity-based knowledge conflicts in question answering.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2021.

\bibitem[Malinin and Gales(2020)]{MG2020}
Andrey Malinin and Mark Gales.
\newblock Uncertainty estimation in autoregressive structured prediction.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2020.

\bibitem[Manakul et~al.(2023)Manakul, Liusie, and Gales]{MLG2023}
Potsawee Manakul, Adian Liusie, and Mark J.~F. Gales.
\newblock {SelfCheckGPT}: Zero-resource black-box hallucination detection for
  generative large language models.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2023.

\bibitem[McAllester and Schapire(2000)]{mcallester2000convergence}
David~A McAllester and Robert~E Schapire.
\newblock On the convergence rate of good-turing estimators.
\newblock In \emph{Conference on Computational Learning Theory (COLT)}, 2000.

\bibitem[Mielke et~al.(2022)Mielke, Szlam, Dinan, and
  Boureau]{mielke2022reducing}
Sabrina~J. Mielke, Arthur Szlam, Emily Dinan, and Y-Lan Boureau.
\newblock Reducing conversational agents' overconfidence through linguistic
  calibration.
\newblock In \emph{Transactions of the Association for Computational
  Linguistics (ACL)}, 2022.

\bibitem[Min et~al.(2020)Min, Michael, Hajishirzi, and
  Zettlemoyer]{min2020ambigqa}
Sewon Min, Julian Michael, Hannaneh Hajishirzi, and Luke Zettlemoyer.
\newblock {A}mbig{QA}: Answering ambiguous open-domain questions.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2020.

\bibitem[Neal(2012)]{Neal2012}
R.~M. Neal.
\newblock Bayesian learning for neural networks.
\newblock \emph{Springer Science \& Business Media}, 2012.

\bibitem[Neeman et~al.(2022)Neeman, Aharoni, Honovich, Choshen, Szpektor, and
  Abend]{neeman2022disentqa}
Ella Neeman, Roee Aharoni, Or~Honovich, Leshem Choshen, Idan Szpektor, and Omri
  Abend.
\newblock Disentqa: Disentangling parametric and contextual knowledge with
  counterfactual question answering.
\newblock \emph{arXiv preprint arXiv:2211.05655}, 2022.

\bibitem[Ohannessian and Dahleh(2010)]{ohannessian2010distribution}
Mesrob~I Ohannessian and Munther~A Dahleh.
\newblock Distribution-dependent performance of the good-turing estimator for
  the missing mass.
\newblock In \emph{19th International Symposium on Mathematical Theory of
  Networks and Systems, MTNS}, 2010.

\bibitem[Osband et~al.(2016)Osband, Blundell, Pritzel, and Van~Roy]{OV2015}
Ian Osband, Charles Blundell, Alexander Pritzel, and Benjamin Van~Roy.
\newblock Deep exploration via bootstrapped dqn.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, volume~29, 2016.

\bibitem[Osband et~al.(2023)Osband, Wen, Asghari, Dwaracherla, Ibrahimi, Lu,
  and Roy]{osband2023epistemic}
Ian Osband, Zheng Wen, Seyed~Mohammad Asghari, Vikranth Dwaracherla, Morteza
  Ibrahimi, Xiuyuan Lu, and Benjamin~Van Roy.
\newblock Epistemic neural networks.
\newblock In \emph{Conference on Neural Information Processing Systems
  (NeurIPS)}, 2023.

\bibitem[Piantadosi(2014)]{piantadosi2014zipf}
Steven~T Piantadosi.
\newblock Zipfâs word frequency law in natural language: A critical review
  and future directions.
\newblock \emph{Psychonomic bulletin \& review}, 21:\penalty0 1112--1130, 2014.

\bibitem[Polyanskiy and Wu(2024)]{polyanskiy2024information}
Yury Polyanskiy and Yihong Wu.
\newblock \emph{Information theory: From coding to learning}.
\newblock Cambridge University Press, 2024.

\bibitem[Rabanser et~al.(2022)Rabanser, Thudi, Hamidieh, Dziedzic, and
  Papernot]{rabanser2022selective}
Stephan Rabanser, Anvith Thudi, Kimia Hamidieh, Adam Dziedzic, and Nicolas
  Papernot.
\newblock Selective classification via neural network training dynamics.
\newblock \emph{arXiv preprint arXiv:2205.13532}, 2022.

\bibitem[Ravfogel et~al.(2023)Ravfogel, Goldberg, and
  Goldberger]{RavfogelACL2023}
Shauli Ravfogel, Yoav Goldberg, and Jacob Goldberger.
\newblock Conformal nucleus sampling.
\newblock In \emph{Transactions of the Association for Computational
  Linguistics (ACL)}, pages 27--34. Association for Computational Linguistics,
  2023.

\bibitem[Tibshirani and Efron(1993)]{tibshirani1993introduction}
Robert~J Tibshirani and Bradley Efron.
\newblock An introduction to the bootstrap.
\newblock \emph{Monographs on statistics and applied probability}, 57\penalty0
  (1), 1993.

\bibitem[Wang et~al.(2022)Wang, Wei, Schuurmans, Le, Chi, Narang, Chowdhery,
  and Zhou]{wang2023selfconsistency}
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc~V Le, Ed~H Chi, Sharan Narang,
  Aakanksha Chowdhery, and Denny Zhou.
\newblock Self-consistency improves chain of thought reasoning in language
  models.
\newblock In \emph{International Conference on Learning Representations
  (ICLR)}, 2022.

\bibitem[Wang and Holmes(2024)]{wang2024subjectiveuncertainty}
Ziyu Wang and Chris Holmes.
\newblock On subjective uncertainty quantification and calibration in natural
  language generation, 2024.
\newblock URL \url{https://arxiv.org/abs/2406.05213}.

\bibitem[Wen et~al.(2022)Wen, Osband, Qin, Lu, Ibrahimi, Dwaracherla, Asghari,
  and Roy]{wen2022predictions}
Zheng Wen, Ian Osband, Chao Qin, Xiuyuan Lu, Morteza Ibrahimi, Vikranth
  Dwaracherla, Mohammad Asghari, and Benjamin~Van Roy.
\newblock From predictions to decisions: The importance of joint predictive
  distributions.
\newblock \emph{arXiv preprint arXiv:2107.09224}, 2022.

\bibitem[Yadkori et~al.(2024)Yadkori, Kuzborskij, Stutz, Gy\"{o}rgy, Fisch,
  Doucet, Beloshapka, Weng, Yang, Szepesv\'{a}ri, Cemgil, and
  Tomasev]{conformal-abstention-2024}
Yasin~Abbasi Yadkori, Ilja Kuzborskij, David Stutz, Andr\'{a}s Gy\"{o}rgy, Adam
  Fisch, Arnaud Doucet, Iuliya Beloshapka, Wei-Hung Weng, Yao-Yuan Yang, Csaba
  Szepesv\'{a}ri, Ali~Taylan Cemgil, and Nenad Tomasev.
\newblock Mitigating llm hallucinations via conformal abstention.
\newblock \emph{arXiv preprint arXiv:2405.01563}, 2024.

\bibitem[Yin et~al.(2024)Yin, Srinivasa, and Chang]{yin2024characterizing}
Fan Yin, Jayanth Srinivasa, and Kai-Wei Chang.
\newblock Characterizing truthfulness in large language model generations with
  local intrinsic dimension.
\newblock In \emph{Forty-first International Conference on Machine Learning},
  2024.
\newblock URL \url{https://openreview.net/forum?id=7DbIyQlfaO}.

\bibitem[Yona et~al.(2024)Yona, Aharoni, and Geva]{Yona2024narrowing}
Gal Yona, Roee Aharoni, and Mor Geva.
\newblock Narrowing the knowledge evaluation gap: Open-domain question
  answering with multi-granularity answers.
\newblock \emph{arXiv preprint arXiv:2401.04695}, 2024.

\bibitem[Zhang et~al.(2023)Zhang, Li, Das, Malin, and Kumar]{ZLDMK-2023}
Jiaxin Zhang, Zhuohang Li, Kamalika Das, Bradley Malin, and Sricharan Kumar.
\newblock Sac3: Reliable hallucination detection in black-box language models
  via semantic-aware cross-check consistency.
\newblock In \emph{Conference on Empirical Methods in Natural Language
  Processing (EMNLP)}, 2023.

\bibitem[Zhao et~al.(2021)Zhao, Wallace, Feng, Klein, and Singh]{Zhao-2021}
Tony~Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh.
\newblock Calibrate before use: Improving few-shot performance of language
  models.
\newblock In \emph{In International Conference on Machine Learning}, 2021.

\bibitem[Zhao et~al.(2024)Zhao, Yan, Sun, Xing, Meng, Wang, Cheng, Ren, and
  Yin]{zhao2024knowing}
Yukun Zhao, Lingyong Yan, Weiwei Sun, Guoliang Xing, Chong Meng, Shuaiqiang
  Wang, Zhicong Cheng, Zhaochun Ren, and Dawei Yin.
\newblock Knowing what {LLM}s do not know: A simple yet effective
  self-detection method.
\newblock \emph{arXiv preprint arXiv:2310.17918}, 2024.

\end{thebibliography}
